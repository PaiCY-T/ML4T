"""
Statistical Feature Selection Engine Demo - Task #29 Stream A

This script demonstrates the Statistical Selection Engine capabilities
for reducing large feature sets from OpenFE to optimal subsets while
preserving information content and eliminating multicollinearity.

Key Demonstrations:
- Processing 500+ features efficiently
- Memory-optimized correlation analysis with VIF
- Mutual information ranking for feature-target relationships  
- Statistical significance testing with multiple test correction
- Information preservation tracking
- Taiwan market compliance validation
"""

import sys
import warnings
from pathlib import Path

# Add project root to path
project_root = Path(__file__).parent.parent
sys.path.append(str(project_root))

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from datetime import datetime, timedelta
import logging

# Configure logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# Import statistical selection engine
from src.feature_selection.statistical import StatisticalSelectionEngine

def generate_synthetic_taiwan_market_data(n_samples=1000, n_features=600):
    """
    Generate synthetic Taiwan stock market data with realistic patterns.
    
    Simulates features that would be generated by OpenFE including:
    - Technical indicators
    - Fundamental ratios
    - Cross-sectional features
    - Time-series features
    - Interaction features
    """
    np.random.seed(42)
    logger.info(f"Generating synthetic data: {n_samples} samples, {n_features} features")
    
    features = {}
    
    # 1. Technical Indicators (150 features)
    logger.info("Generating technical indicators...")
    for i in range(150):
        # Simulate moving averages, RSI, MACD, etc.
        if i < 50:
            # Moving averages with different windows
            base_price = 100 + np.random.randn(n_samples).cumsum() * 0.5
            window = 5 + (i % 20)
            ma = pd.Series(base_price).rolling(window=window, min_periods=1).mean()
            features[f'MA_{window}_{i}'] = ma.values
        elif i < 100:
            # Momentum indicators
            base_returns = np.random.randn(n_samples) * 0.02
            momentum = pd.Series(base_returns).rolling(window=10, min_periods=1).sum()
            features[f'momentum_{i-50}'] = momentum.values
        else:
            # Volatility indicators
            base_vol = np.abs(np.random.randn(n_samples)) * 0.01
            vol_ma = pd.Series(base_vol).rolling(window=20, min_periods=1).mean()
            features[f'volatility_{i-100}'] = vol_ma.values
            
    # 2. Fundamental Ratios (100 features)
    logger.info("Generating fundamental ratios...")
    for i in range(100):
        if i < 25:
            # P/E ratios (with some outliers)
            pe_ratio = np.abs(np.random.lognormal(2.5, 0.5, n_samples))
            features[f'PE_ratio_{i}'] = np.clip(pe_ratio, 1, 100)  # Cap extreme values
        elif i < 50:
            # Price-to-Book ratios
            pb_ratio = np.abs(np.random.lognormal(0.5, 0.3, n_samples))
            features[f'PB_ratio_{i-25}'] = pb_ratio
        elif i < 75:
            # ROE, ROA ratios
            roe = np.random.normal(0.12, 0.08, n_samples)
            features[f'ROE_{i-50}'] = roe
        else:
            # Debt ratios
            debt_ratio = np.abs(np.random.beta(2, 5, n_samples))
            features[f'debt_ratio_{i-75}'] = debt_ratio
            
    # 3. Cross-sectional Features (150 features)
    logger.info("Generating cross-sectional features...")
    for i in range(150):
        if i < 50:
            # Market cap quintiles effect
            market_cap = np.random.lognormal(10, 2, n_samples)
            quintiles = pd.qcut(market_cap, 5, labels=False, duplicates='drop')
            features[f'market_cap_quintile_{i}'] = quintiles
        elif i < 100:
            # Sector effects (simulate 10 sectors)
            sector = np.random.randint(0, 10, n_samples)
            features[f'sector_effect_{i-50}'] = sector + np.random.randn(n_samples) * 0.1
        else:
            # Industry momentum
            industry_mom = np.random.randn(n_samples) * 0.03
            features[f'industry_momentum_{i-100}'] = industry_mom
            
    # 4. Time-series Features (100 features) 
    logger.info("Generating time-series features...")
    for i in range(100):
        # Lagged features with different lags
        base_series = np.random.randn(n_samples).cumsum() * 0.1
        lag = (i % 20) + 1
        lagged = np.roll(base_series, lag)
        lagged[:lag] = 0  # Set initial values to 0
        features[f'lagged_feature_{lag}_{i}'] = lagged
        
    # 5. Interaction/OpenFE Generated Features (100 features)
    logger.info("Generating OpenFE-style interaction features...")
    # Select some base features for interactions
    base_features = [features[f'MA_5_{i}'] for i in range(min(10, len(features)))]
    
    for i in range(100):
        if i < 50 and len(base_features) >= 2:
            # Polynomial features
            idx1, idx2 = np.random.choice(len(base_features), 2, replace=False)
            poly_feature = base_features[idx1] * base_features[idx2]
            features[f'poly_interaction_{i}'] = poly_feature
        elif i < 75:
            # Ratio features
            numerator = np.abs(np.random.randn(n_samples)) + 1e-6
            denominator = np.abs(np.random.randn(n_samples)) + 1e-6
            ratio = numerator / denominator
            features[f'ratio_feature_{i-50}'] = ratio
        else:
            # Log transforms
            base_positive = np.abs(np.random.randn(n_samples)) + 1
            log_feature = np.log(base_positive)
            features[f'log_transform_{i-75}'] = log_feature
            
    # Add some highly correlated features (should be filtered out)
    logger.info("Adding correlated features for testing...")
    base_corr_feature = features['MA_5_0']
    for i in range(20):
        corr_feature = base_corr_feature + np.random.randn(n_samples) * 0.01
        features[f'highly_correlated_{i}'] = corr_feature
        
    # Add some constant/quasi-constant features (should be filtered out)
    for i in range(10):
        const_feature = np.ones(n_samples) * (10 + i) + np.random.randn(n_samples) * 0.001
        features[f'quasi_constant_{i}'] = const_feature
        
    # Create DataFrame
    df = pd.DataFrame(features)
    logger.info(f"Generated feature matrix: {df.shape}")
    
    return df

def generate_target_returns(n_samples=1000):
    """Generate synthetic target returns with realistic Taiwan market characteristics."""
    np.random.seed(42)
    logger.info(f"Generating target returns for {n_samples} samples")
    
    # Taiwan market characteristics
    annual_return = 0.08  # 8% annual return
    annual_volatility = 0.20  # 20% annual volatility
    daily_return = annual_return / 252
    daily_volatility = annual_volatility / np.sqrt(252)
    
    # Generate returns with volatility clustering
    returns = []
    vol = daily_volatility
    
    for i in range(n_samples):
        if i > 0:
            # GARCH-like volatility updating
            vol = 0.1 * daily_volatility + 0.85 * vol + 0.05 * abs(returns[i-1])
            vol = min(vol, daily_volatility * 3)  # Cap extreme volatility
            
        # Generate return
        ret = np.random.normal(daily_return, vol)
        returns.append(ret)
        
    # Add some market regime changes
    regime_changes = [200, 400, 600, 800]
    for change_point in regime_changes:
        if change_point < n_samples:
            # Simulate market stress
            stress_period = 20
            for j in range(min(stress_period, n_samples - change_point)):
                returns[change_point + j] *= 2  # Higher volatility
                
    target = pd.Series(returns, name='forward_returns')
    
    logger.info(f"Target statistics - Mean: {target.mean():.4f}, Std: {target.std():.4f}")
    return target

def demonstrate_statistical_selection():
    """Main demonstration of the Statistical Selection Engine."""
    print("="*80)
    print("STATISTICAL FEATURE SELECTION ENGINE DEMONSTRATION")
    print("Task #29 Stream A - Taiwan Stock Market Feature Selection")
    print("="*80)
    
    # Generate synthetic data
    print("\n1. GENERATING SYNTHETIC TAIWAN MARKET DATA")
    print("-" * 50)
    
    n_samples = 1000
    n_features = 600  # Simulate OpenFE output
    
    X = generate_synthetic_taiwan_market_data(n_samples, n_features)
    y = generate_target_returns(n_samples)
    
    print(f"✓ Generated {X.shape[1]} features for {X.shape[0]} samples")
    print(f"✓ Target returns: {len(y)} samples")
    print(f"✓ Memory usage: ~{X.memory_usage(deep=True).sum() / 1024**2:.1f} MB")
    
    # Initialize Statistical Selection Engine
    print("\n2. INITIALIZING STATISTICAL SELECTION ENGINE") 
    print("-" * 50)
    
    engine = StatisticalSelectionEngine(
        target_feature_count=75,           # Target: 75 features
        min_feature_count=50,              # Minimum: 50 features  
        max_feature_count=100,             # Maximum: 100 features
        correlation_threshold=0.7,         # Max correlation: 0.7
        vif_threshold=10.0,               # Max VIF: 10
        variance_threshold=0.01,           # Min variance: 0.01
        mi_threshold=0.01,                # Min mutual info: 0.01
        significance_alpha=0.05,           # Significance level: 0.05
        memory_limit_gb=8.0,              # Memory limit: 8GB
        preserve_info_threshold=0.9        # Info preservation: 90%
    )
    
    print("✓ Engine initialized with Taiwan market parameters")
    print(f"  - Target features: {engine.target_feature_count}")
    print(f"  - Correlation threshold: {engine.correlation_threshold}")
    print(f"  - VIF threshold: {engine.vif_threshold}")
    print(f"  - Memory limit: {engine.memory_limit_gb} GB")
    
    # Run statistical selection pipeline
    print("\n3. RUNNING STATISTICAL SELECTION PIPELINE")
    print("-" * 50)
    
    start_time = datetime.now()
    
    try:
        # Fit the statistical selection engine
        engine.fit_statistical_selection(X, y, save_intermediate=True)
        
        elapsed_time = (datetime.now() - start_time).total_seconds()
        
        print(f"✓ Pipeline completed in {elapsed_time:.1f} seconds")
        print(f"✓ Memory peak: {engine.final_stats_['memory_peak_gb']:.2f} GB")
        
    except Exception as e:
        print(f"✗ Pipeline failed: {str(e)}")
        return
    
    # Display results
    print("\n4. SELECTION RESULTS SUMMARY")
    print("-" * 50)
    
    results = engine.get_selection_summary()
    final_stats = results['final_statistics']
    
    print(f"Original features:     {final_stats['original_feature_count']}")
    print(f"Selected features:     {final_stats['final_feature_count']}")  
    print(f"Reduction ratio:       {final_stats['reduction_ratio']:.1%}")
    print(f"Information preserved: {final_stats['information_preservation']:.1%}")
    print(f"Processing time:       {final_stats['processing_time_seconds']:.1f}s")
    print(f"Target achieved:       {final_stats['target_achieved']}")
    
    # Stage-by-stage breakdown
    print("\n5. STAGE-BY-STAGE BREAKDOWN")
    print("-" * 50)
    
    stage_names = {
        'variance_filtering': 'Variance Filtering',
        'correlation_analysis': 'Correlation & VIF Analysis', 
        'mutual_information': 'Mutual Information Ranking',
        'significance_testing': 'Statistical Significance Testing'
    }
    
    for stage_key, stage_name in stage_names.items():
        if stage_key in results['stage_results']:
            stage_info = results['stage_results'][stage_key]
            if not stage_info.get('skipped'):
                print(f"{stage_name}:")
                print(f"  Input features:  {stage_info.get('input_features', 'N/A')}")
                print(f"  Output features: {stage_info.get('output_features', 'N/A')}")
                print(f"  Eliminated:      {len(stage_info.get('eliminated', {}))}")
                print(f"  Processing time: {stage_info.get('processing_time', 0):.1f}s")
                print()
    
    # Feature importance analysis
    print("\n6. FEATURE IMPORTANCE ANALYSIS")
    print("-" * 50)
    
    importance_report = engine.get_feature_importance_report(top_n=10)
    
    print("Top 10 Selected Features:")
    for i, (feature, score) in enumerate(importance_report['top_features'][:10], 1):
        print(f"  {i:2}. {feature:<30} Score: {score:.4f}")
        
    print(f"\nElimination Reasons Breakdown:")
    for reason, count in importance_report['elimination_reasons'].items():
        if count > 0:
            print(f"  {reason.replace('_', ' ').title()}: {count} features")
    
    # Memory usage analysis
    print("\n7. MEMORY USAGE ANALYSIS")
    print("-" * 50)
    
    memory_stats = results['memory_usage']
    if memory_stats:
        memory_values = [stats['memory_gb'] for stats in memory_stats.values()]
        print(f"Memory usage - Peak: {max(memory_values):.2f} GB")
        print(f"Memory usage - Average: {np.mean(memory_values):.2f} GB")
        
        # Show memory progression
        key_stages = ['pipeline_start', 'stage1_complete', 'stage2_complete', 
                     'stage3_complete', 'stage4_complete', 'pipeline_complete']
        
        print("\nMemory progression by stage:")
        for stage in key_stages:
            if stage in memory_stats:
                memory_gb = memory_stats[stage]['memory_gb']
                print(f"  {stage.replace('_', ' ').title():<25}: {memory_gb:.2f} GB")
    
    # Validation and quality checks
    print("\n8. VALIDATION & QUALITY CHECKS")
    print("-" * 50)
    
    # Check feature count targets
    selected_count = len(engine.selected_features_)
    print(f"✓ Feature count within range: {engine.min_feature_count} ≤ {selected_count} ≤ {engine.max_feature_count}")
    
    # Check multicollinearity elimination
    if 'correlation_analysis' in results['stage_results']:
        corr_stats = results['stage_results']['correlation_analysis']
        print(f"✓ High VIF features eliminated: {corr_stats.get('high_vif_features', 0)}")
        print(f"✓ Correlation clusters formed: {corr_stats.get('clusters_formed', 0)}")
    
    # Check information preservation
    info_preservation = final_stats['information_preservation']
    preservation_target = engine.preserve_info_threshold
    if info_preservation >= preservation_target:
        print(f"✓ Information preservation: {info_preservation:.1%} ≥ {preservation_target:.1%}")
    else:
        print(f"⚠ Information preservation: {info_preservation:.1%} < {preservation_target:.1%}")
    
    # Processing time validation
    if final_stats['processing_time_seconds'] < 1800:  # 30 minutes
        print(f"✓ Processing time: {final_stats['processing_time_seconds']:.1f}s < 30min")
    else:
        print(f"⚠ Processing time: {final_stats['processing_time_seconds']:.1f}s > 30min")
    
    # Transform demonstration
    print("\n9. FEATURE TRANSFORMATION DEMO")
    print("-" * 50)
    
    # Transform original data
    X_transformed = engine.transform(X)
    print(f"✓ Original data transformed: {X.shape} → {X_transformed.shape}")
    print(f"✓ Selected features: {list(X_transformed.columns)[:5]}... (showing first 5)")
    
    # Simulate new data transformation
    new_data = X.iloc[:100].copy()  # Simulate new batch
    X_new_transformed = engine.transform(new_data)
    print(f"✓ New data transformed: {new_data.shape} → {X_new_transformed.shape}")
    
    # Final summary
    print("\n" + "="*80)
    print("STATISTICAL SELECTION PIPELINE COMPLETED SUCCESSFULLY")
    print(f"Reduced {X.shape[1]} features to {len(engine.selected_features_)} optimal features")
    print(f"Information preservation: {info_preservation:.1%}")
    print(f"Processing time: {final_stats['processing_time_seconds']:.1f} seconds") 
    print(f"Memory efficient: Peak {final_stats.get('memory_peak_gb', 0):.2f} GB")
    print("="*80)
    
    return engine, X_transformed, y

def create_visualization_plots(engine, X_transformed, y):
    """Create visualization plots for the selection results."""
    try:
        import matplotlib.pyplot as plt
        import seaborn as sns
        
        plt.style.use('seaborn-v0_8')
        fig, axes = plt.subplots(2, 2, figsize=(15, 12))
        fig.suptitle('Statistical Feature Selection Results - Task #29 Stream A', fontsize=16)
        
        # 1. Feature selection pipeline flow
        ax1 = axes[0, 0]
        stage_results = engine.stage_results_
        stage_names = ['Original', 'Variance', 'Correlation', 'MI', 'Significance', 'Final']
        
        # Get feature counts at each stage
        original_count = engine.final_stats_['original_feature_count']
        counts = [original_count]
        
        for stage in ['variance_filtering', 'correlation_analysis', 'mutual_information', 'significance_testing']:
            if stage in stage_results and not stage_results[stage].get('skipped'):
                counts.append(stage_results[stage].get('output_features', counts[-1]))
            else:
                counts.append(counts[-1])
        
        counts.append(engine.final_stats_['final_feature_count'])
        
        ax1.plot(range(len(stage_names)), counts, marker='o', linewidth=3, markersize=8)
        ax1.set_xticks(range(len(stage_names)))
        ax1.set_xticklabels(stage_names, rotation=45)
        ax1.set_ylabel('Number of Features')
        ax1.set_title('Feature Selection Pipeline Flow')
        ax1.grid(True, alpha=0.3)
        
        # 2. Top feature scores
        ax2 = axes[0, 1]
        if engine.feature_scores_:
            top_features = sorted(engine.feature_scores_.items(), key=lambda x: x[1], reverse=True)[:15]
            features, scores = zip(*top_features)
            
            y_pos = range(len(features))
            bars = ax2.barh(y_pos, scores)
            ax2.set_yticks(y_pos)
            ax2.set_yticklabels([f[:20] + '...' if len(f) > 20 else f for f in features])
            ax2.set_xlabel('Combined Score')
            ax2.set_title('Top 15 Feature Scores')
            
            # Color bars by score
            for bar, score in zip(bars, scores):
                bar.set_color(plt.cm.viridis(score / max(scores)))
        
        # 3. Elimination reasons breakdown
        ax3 = axes[1, 0]
        elimination_reasons = {}
        for reason in engine.elimination_history_.values():
            key = reason.split('_')[1] if '_' in reason else 'other'
            elimination_reasons[key] = elimination_reasons.get(key, 0) + 1
            
        if elimination_reasons:
            reasons = list(elimination_reasons.keys())
            counts = list(elimination_reasons.values())
            
            wedges, texts, autotexts = ax3.pie(counts, labels=reasons, autopct='%1.1f%%', startangle=90)
            ax3.set_title('Feature Elimination Reasons')
        
        # 4. Feature correlation heatmap (sample)
        ax4 = axes[1, 1]
        if X_transformed.shape[1] > 1:
            # Sample features if too many
            sample_features = X_transformed.columns[:min(10, X_transformed.shape[1])]
            corr_matrix = X_transformed[sample_features].corr()
            
            sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', center=0, 
                       square=True, ax=ax4, fmt='.2f', cbar_kws={'shrink': 0.8})
            ax4.set_title('Selected Features Correlation Matrix\n(Sample of up to 10 features)')
        
        plt.tight_layout()
        plt.savefig('statistical_selection_results.png', dpi=300, bbox_inches='tight')
        print(f"\n✓ Visualization saved as 'statistical_selection_results.png'")
        plt.show()
        
    except ImportError:
        print("⚠ Matplotlib/Seaborn not available - skipping visualizations")
    except Exception as e:
        print(f"⚠ Visualization error: {str(e)}")

if __name__ == "__main__":
    # Suppress warnings for cleaner output
    warnings.filterwarnings('ignore')
    
    try:
        # Run demonstration
        engine, X_transformed, y = demonstrate_statistical_selection()
        
        # Create visualizations
        create_visualization_plots(engine, X_transformed, y)
        
        print("\n🎉 Demo completed successfully!")
        print("Check the generated files and visualizations for detailed results.")
        
    except Exception as e:
        print(f"\n❌ Demo failed with error: {str(e)}")
        import traceback
        traceback.print_exc()
        
    print("\nEnd of Statistical Feature Selection Demo")